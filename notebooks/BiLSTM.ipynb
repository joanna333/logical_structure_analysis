{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from nlpaug.augmenter.word import BackTranslationAug, RandomWordAug, ContextualWordEmbsAug\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.flow as naf  \n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111f4f5",
   "metadata": {},
   "source": [
    "# 1. Setup and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb93c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        \n",
    "        # Create label encoder if not already encoded\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(labels)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194dfd7",
   "metadata": {},
   "source": [
    "# 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttentionBERT(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_classes=22, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Improved Architecture\n",
    "        self.bert_model = AutoModel.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "\n",
    "        self.dropout_bert = nn.Dropout(dropout)  # Added this line\n",
    "        \n",
    "        # Freeze initial BERT layers\n",
    "        for param in list(self.bert_model.parameters())[:-2]:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 2. Bidirectional LSTM with residual connections\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 3. Enhanced Attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2,\n",
    "            num_heads=8,  # Increased heads\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 4. Improved Regularization\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout + 0.1)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        \n",
    "        # 5. Deeper Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout + 0.1),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERT encoding with dropout\n",
    "        bert_output = self.bert_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        sequence_output = self.dropout_bert(bert_output.last_hidden_state)\n",
    "        \n",
    "        # BiLSTM with layer norm\n",
    "        lstm_out, _ = self.lstm(sequence_output)\n",
    "        lstm_out = self.layer_norm1(lstm_out)  # First layer norm\n",
    "        \n",
    "        # Self-attention with dropout\n",
    "        attn_out, _ = self.attention(\n",
    "            query=lstm_out,\n",
    "            key=lstm_out,\n",
    "            value=lstm_out,\n",
    "            need_weights=False\n",
    "        )\n",
    "        attn_out = self.dropout1(attn_out)  # First dropout\n",
    "        attn_out = self.layer_norm2(attn_out)  # Second layer norm\n",
    "        \n",
    "        # Pooling and normalization\n",
    "        pooled = torch.mean(attn_out, dim=1)\n",
    "        pooled = self.batch_norm(pooled)  # Batch norm\n",
    "        pooled = self.dropout2(pooled)  # Second dropout\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(pooled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd6191",
   "metadata": {},
   "source": [
    "# 3. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Define file paths\n",
    "    data_files = [\n",
    "        'data/processed/all_labeled_sentences/combined_labeled_sentences.csv',\n",
    "        'data/processed/all_labeled_sentences/combined_new_labeled_sentences.csv',\n",
    "        'data/processed/all_labeled_sentences/combined_sentence_types.csv',\n",
    "        'data/processed/all_labeled_sentences/combined_ai_data.csv'\n",
    "    ]\n",
    "    \n",
    "    # Load and combine all files\n",
    "    dfs = []\n",
    "    for file in data_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            print(f\"Loaded {file}: {len(df)} rows\")\n",
    "            dfs.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {file} not found\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates if any\n",
    "    combined_df = combined_df.drop_duplicates(subset=['Sentence'])\n",
    "    \n",
    "    print(f\"Total combined dataset size: {len(combined_df)}\")\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143169f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data_loaders(df, batch_size=16, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SentenceDataset(train_df['Sentence'], train_df['Label'], tokenizer)\n",
    "    val_dataset = SentenceDataset(val_df['Sentence'], val_df['Label'], tokenizer)\n",
    "    \n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca208fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(num_epochs=1, learning_rate=2e-5, weight_decay=0.01, device=None):\n",
    "    \"\"\"Training function for BiLSTM model with BERT embeddings.\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "    df = load_data()\n",
    "    \n",
    "    # Data augmentation\n",
    "    # augmented_df = apply_augmentation(df)\n",
    "    # train_loader, val_loader = create_data_loaders(augmented_df, tokenizer=tokenizer)\n",
    "    \n",
    "    train_loader, val_loader = create_data_loaders(df, tokenizer=tokenizer)\n",
    "\n",
    "    # Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = df['Label'].values\n",
    "    label_encoder.fit(labels)\n",
    "    \n",
    "    # Model initialization\n",
    "    model = BiLSTMAttentionBERT(\n",
    "        hidden_dim=256,\n",
    "        num_classes=len(label_encoder.classes_),\n",
    "        num_layers=2,\n",
    "        dropout=0.5\n",
    "    )\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Loss with label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "    \n",
    "    # Training tracking\n",
    "    best_val_acc = 0\n",
    "    grad_accumulation_steps = 2\n",
    "    wandb.init(\n",
    "    project=\"bilstm-classification\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": num_epochs\n",
    "        }\n",
    "    )\n",
    "\n",
    "    wandb.config.update({\n",
    "        \n",
    "        # Dataset Stats\n",
    "        \"train_size\": len(train_loader.dataset),\n",
    "        \"val_size\": len(val_loader.dataset),\n",
    "        \"num_classes\": len(label_encoder.classes_),\n",
    "        \n",
    "        # Training Config\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": scheduler.__class__.__name__ if scheduler else \"none\",\n",
    "    })\n",
    "\n",
    "    # At the start of training\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Training phase\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            loss = loss / grad_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if (batch_idx + 1) % grad_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * grad_accumulation_steps\n",
    "\n",
    "            # Inside training loop, after computing loss\n",
    "            train_pred = torch.argmax(outputs, dim=1)\n",
    "            train_acc = (train_pred == labels).float().mean()\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_best_model(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch,\n",
    "                val_acc=val_acc,\n",
    "                filename=f'models/model_epoch{epoch}_acc{val_acc:.2f}.pt',\n",
    "                label_encoder=label_encoder\n",
    "            )\n",
    "        \n",
    "        # After each epoch\n",
    "        train_loss = total_loss/len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Print epoch stats\n",
    "        print_and_handle_metrics(\n",
    "            epoch=epoch,\n",
    "            num_epochs=num_epochs,\n",
    "            train_loss=total_loss/len(train_loader),\n",
    "            train_acc=train_acc,\n",
    "            val_loss=val_loss,\n",
    "            val_acc=val_acc,\n",
    "            best_val_acc=best_val_acc,\n",
    "            early_stopping_patience=early_stopping.patience\n",
    "        )\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"loss_gap\": train_loss - val_loss,\n",
    "            \"accuracy_gap\": train_acc - val_acc,\n",
    "            \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
    "        }, step=epoch)\n",
    "        train_accuracies = train_accuracies.cpu() \n",
    "        val_accuracies = val_accuracies.cpu()\n",
    "        train_losses = train_losses.cpu()\n",
    "        val_losses = val_losses.cpu()\n",
    "        # Plot learning curves\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss Curves')\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(train_accuracies, label='Train Acc')\n",
    "        plt.plot(val_accuracies, label='Val Acc')\n",
    "        plt.legend()\n",
    "        plt.title('Accuracy Curves')\n",
    "        wandb.log({\"learning_curves\": wandb.Image(plt)})\n",
    "\n",
    "    wandb.finish()\n",
    "    return model, train_loader, val_loader, best_val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1a3dd",
   "metadata": {},
   "source": [
    "# 4. Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            return None, None\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass with gradient clipping\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a702af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on validation set.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to evaluate\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        device: Device to run evaluation on (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average loss, accuracy percentage)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Store batch results\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * np.mean(np.array(predictions) == np.array(true_labels))\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "    \n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d68a03",
   "metadata": {},
   "source": [
    "# Early stopping implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_and_handle_metrics(epoch, num_epochs, train_loss, train_acc, val_loss, val_acc, best_val_acc, early_stopping_patience):\n",
    "    \"\"\"\n",
    "    Print training metrics and return updated best validation accuracy.\n",
    "    \"\"\"\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "    print(f'Best Validation Accuracy: {best_val_acc:.4f}')\n",
    "    #print(f'Early Stopping Patience Remaining: {early_stopping_patience-no_improve_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_best_model(model, optimizer, epoch, val_acc, filename, label_encoder):\n",
    "    \"\"\"\n",
    "    Save the model checkpoint with relevant training information.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to save\n",
    "        optimizer: The optimizer used for training\n",
    "        epoch: Current epoch number\n",
    "        val_acc: Validation accuracy\n",
    "        filename: Path where to save the model\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(f\"Saving model with {len(label_encoder.classes_)} classes\")\n",
    "    print(\"Classes:\", label_encoder.classes_)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "    tokenizer.save_pretrained(os.path.join('models', 'tokenizer'))\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'val_acc': val_acc,\n",
    "        'label_encoder_classes': label_encoder.classes_,\n",
    "        'num_classes': len(label_encoder.classes_)\n",
    "    }, filename, _use_new_zipfile_serialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9052e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_saved_model(model, optimizer, filename):\n",
    "    \"\"\"\n",
    "    Load a saved model checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: The model architecture to load weights into\n",
    "        optimizer: The optimizer to load state into\n",
    "        filename: Path to the saved model checkpoint\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded model\n",
    "        optimizer: Loaded optimizer\n",
    "        epoch: The epoch where training stopped\n",
    "        val_acc: The validation accuracy at save time\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    val_acc = checkpoint['val_acc']\n",
    "    return model, optimizer, epoch, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe337a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model, train_loader, val_loader, best_val_acc = train_model()\n",
    "if model is not None:\n",
    "    print(f\"Training completed with best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
